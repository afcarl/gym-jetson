{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Reinforcement Learning with NVIDIA Jetson TX2\n",
    "\n",
    "In this session, you will use a branch of machine learning called **reinforcement learning** (RL) to teach a robot to play a game."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Game\n",
    "\n",
    "The setup has four LEDs.  We enumerate the LEDs starting with zero, so that the yellow LED is at position `0`, the red LED is at position `1`, and so on.  Each LED is connected to a button that is used to turn it off.  \n",
    "\n",
    "![LEDs](images/LEDs.png)\n",
    "\n",
    "The game round begins when **one** of the four LEDs is turned on, and the robotic arm starts at a position hovering over one of the four buttons.  The score always starts at zero.\n",
    "\n",
    "At each point in the game, the robot has three potential movements (or **actions**) at its disposal.  It can:\n",
    "- `0` - move **L**eft one position,\n",
    "- `1` - stay at the current position and **P**ush, or\n",
    "- `2` - move **R**ight one position.\n",
    "\n",
    "After the robot chooses an action, its score is deducted one point.  While most actions have intuitive effects, there are a few special cases that are worth mentioning:\n",
    "- The lit LED is turned off when the robot pushes its corresponding button, but pushing buttons connected to unlit LEDs has no effect (i.e., they are never turned on by the robot).  \n",
    "- If the robotic arm is hovering over the leftmost button at location `0` and decides to move left, we imagine that the arm hits an imaginary wall, and the arm stays where it is.  Likewise, if the arm is hovering over the button at position `3` and decides to move right, then at the next point in the game, the arm will have maintained its position at location `3` (the rightmost location in the line).\n",
    "\n",
    "The round ends when the robot pushes the correct button to turn off the lit LED.  Your goal in this notebook is to implement an algorithm that can learn from gameplay the optimal strategy to attain the highest (or least negative) score at each round.  To accomplish this, your robot will need to learn how to turn off the lit LED in as few movements as possible.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Motivating Reinforcement Learning\n",
    "\n",
    "You may have noticed that the winning game strategy is very straightforward to hard code.  With this in mind, if you are new to RL, you might wonder why an RL technique would not be overkill.\n",
    "\n",
    "And the fact is that in the field of RL, it is very common to use simple games with well-defined rules **to build intuition** for how to design algorithms to best accomplish more complex tasks.  \n",
    "\n",
    "In the RL setting, we assume that the robot does not have the domain knowledge to know what \"left\", \"push\", or \"right\" means.  The robot only knows that it has three possible actions, and it needs to figure out how to select from these actions to consistently attain the highest possible score.  \n",
    "\n",
    "You can think of the robot as a computer player with full access to a keyboard that only contains three keys, where you further assume the player does not know any of the game controls.  Despite this missing information, the player must nonetheless learn how to beat the game.  \n",
    "\n",
    "And, similar to how you can imagine the human player would learn, the robot will closely watch its score in the game to gauge how well it is doing.  If it is not performing well, it will amend its strategy to do better.  \n",
    "\n",
    "The algorithm that you will use to determine the best gameplay strategy is remarkably intuitive.  The robot will learn primarily through trial-and-error, while using its score as a feedback mechanism to hone its strategy.  Its initial behavior will be incredibly random, as it tries out many different moves (or actions) in the game, to see how its score is affected.  The main idea behind the algorithm is to train the robot to leverage its gameplay experience to gradually obtain a well-informed strategy that attains a relatively high score.  \n",
    "\n",
    "For now, in the next section, we will examine how the robot performs, if it chooses random actions at each time step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Investigate Random Behavior\n",
    "\n",
    "We have written a simple simulator that you can use to see how the robot should perform, if it selects random actions for the entirety of the game.  Of course, your robot will learn to perform much better!\n",
    "\n",
    "Run the code cell below to have the robot play the game for 2 separate rounds.  When parsing the output, remember that the starting conditions of the game are random!\n",
    "\n",
    "Each round has corresponding output that looks somewhat like the snippet below:\n",
    "```\n",
    "Starting Round 1 ...\n",
    " LED: 0    | Arm: 1 | Score:    0 \n",
    " Action: P | Arm: 1 | Score:   -1 \n",
    " Action: L | Arm: 0 | Score:   -2 \n",
    " Action: P | Arm: 0 | Score:   -3 \n",
    "FINAL SCORE:  -3\n",
    "-----------------------------------\n",
    "```\n",
    "\n",
    "In the sample snippet above:\n",
    "- When the game round was initiated, the lit LED was at position `0`, and the arm was at position `1`.  The game score at the beginning of a round is always zero. \n",
    "- The robot's first choice of action was to **P**ush in the current location; as a result, its score was deducted one point, and the LEDs were unaffected (i.e., the LED at position `0` remains lit, while all of the other LEDs are unlit).  \n",
    "- The robot's next choice was to move **L**eft, so it moved to position `0` and the score was deducted another point.\n",
    "- Then, the robot decided to **P**ush in location `0`, and the game score decreased to `-3`.  At this point, the game ended, because the final action turned off the lit LED. \n",
    "- In this case, the final score received at the end of the game was `-1` + `-1` + `-1` = `-3`.\n",
    "\n",
    "Take the time now to understand the `JetsonEnv` class in **jetson_env.py**.  Note that the simulation encodes each of the possible actions as an integer (one of `0`, `1`, or `2`), and to get the corresponding more interpretable action label (`L`, `P`, or `R`), we use the `decipher_action` function below. \n",
    "\n",
    "Later in this notebook, you will use the `JetsonEnv` class to simulate games to teach your robot!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jetson_env import JetsonEnv\n",
    "\n",
    "# use a Python dictionary to decode the actions\n",
    "action_dict = {0: 'L', 1: 'P', 2: 'R'}\n",
    "def decipher_action(a):\n",
    "    return action_dict[a]\n",
    "\n",
    "# create a new environment\n",
    "env = JetsonEnv()\n",
    "\n",
    "# interact with the environment\n",
    "for i_episode in range(1, 3):\n",
    "    print('Starting Round %d ...' % i_episode)\n",
    "    # reset the lit LED, arm position, and score\n",
    "    led, arm = env.reset()\n",
    "    score = 0\n",
    "    print(' LED: %d    | Arm: %d | Score: %4d ' % (led, arm, score))\n",
    "    while True:\n",
    "        action = env.get_random_action() # select a random action\n",
    "        arm, reward, done = env.step(action)\n",
    "        score += reward\n",
    "        print(' Action: %s | Arm: %d | Score: %4d ' % (decipher_action(action), arm, score))\n",
    "        if done:\n",
    "            print('FINAL SCORE: ', score)\n",
    "            print('-'*35)\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## States, Actions, and Optimal Policies\n",
    "\n",
    "As discovered above, there are **three possible actions**, corresponding to:\n",
    "- `0` - moving **L**eft, \n",
    "- `1` - staying and **P**ushing in the current position, and\n",
    "- `2` - moving **R**ight.\n",
    "\n",
    "The **total number of possible game states is $4^2 = 16$**, where there is a state for each possible combination of arm position and lit LED position. \n",
    "\n",
    "To avoid having to deal with two different numbers when referencing the state, we define the `get_state` function below that maps each possible combination of arm position (`arm`) and lit LED position (`led`) to an integer from `0` to `15`, which we refer to as the corresponding state (`state`).  \n",
    "\n",
    "In your upcoming implementation, the state should always be encoded as a number from `0` to `15`, but you can get the corresponding arm position (`arm`) and lit LED position (`led`) by passing the state (`state`) into the `get_led_and_arm` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_led_and_arm(state, nLED=4):\n",
    "    arm = state % nLED\n",
    "    led = int(((state-arm))/nLED)\n",
    "    return led, arm\n",
    "\n",
    "def get_state(led, arm, nLED=4):\n",
    "    state = led*nLED + arm\n",
    "    return state\n",
    "\n",
    "for state in range(16):\n",
    "    led, arm = get_led_and_arm(state)\n",
    "    print('LED: %d | Arm: %d | State: %2d ' % (led, arm, state))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of your robot is to find - for each possible game state - the best action that the robot should take from that state, towards its goal of maximizing the game score.  We will think of this as a lookup table that the robot can consult when selecting actions, and we refer to it as an **optimal policy**.\n",
    "\n",
    "For instance, consider the case that the game starts in state `0`.  This state corresponds to arm position `0` and lit LED position `0`.  In this case, the robot should select action **P**ush, to end the game with a final score of `-1` immediately after.\n",
    "\n",
    "Likewise, state `1` corresponds to arm position `1` and lit LED position `0`.  In this case, the robot should decide to move **L**eft as the best initial move.  (This way, the robot can select to **P**ush at the next step and end the game with a best final score of `-2`.)\n",
    "\n",
    "Take the time now to look at the printed optimal policy below.  Check to make sure that you can see why these actions are optimal, in the context of their corresponding game states!\n",
    "\n",
    "```\n",
    "LED: 0 | Arm: 0 | State:  0 | Best Action: P\n",
    "LED: 0 | Arm: 1 | State:  1 | Best Action: L\n",
    "LED: 0 | Arm: 2 | State:  2 | Best Action: L\n",
    "LED: 0 | Arm: 3 | State:  3 | Best Action: L\n",
    "LED: 1 | Arm: 0 | State:  4 | Best Action: R\n",
    "LED: 1 | Arm: 1 | State:  5 | Best Action: P\n",
    "LED: 1 | Arm: 2 | State:  6 | Best Action: L\n",
    "LED: 1 | Arm: 3 | State:  7 | Best Action: L\n",
    "LED: 2 | Arm: 0 | State:  8 | Best Action: R\n",
    "LED: 2 | Arm: 1 | State:  9 | Best Action: R\n",
    "LED: 2 | Arm: 2 | State: 10 | Best Action: P\n",
    "LED: 2 | Arm: 3 | State: 11 | Best Action: L\n",
    "LED: 3 | Arm: 0 | State: 12 | Best Action: R\n",
    "LED: 3 | Arm: 1 | State: 13 | Best Action: R\n",
    "LED: 3 | Arm: 2 | State: 14 | Best Action: R\n",
    "LED: 3 | Arm: 3 | State: 15 | Best Action: P\n",
    "```\n",
    "\n",
    "Once the robot has determined the optimal policy, it can reference it when playing the game, in order to consistently attain the highest possible score.  For instance, if the robot is presented with a situation in the game where the lit LED is at position 2, and its arm is at position 1, it need only find the line corresponding to state 9 in the table, where it will see that the best action is to go left.\n",
    "\n",
    "Next, you will implement a method known as **Monte Carlo with Exploring Starts** to guide your robot to obtain this optimal policy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monte Carlo with Exploring Starts\n",
    "\n",
    "As part of this method, the robot will maintain a numpy array `Q` with `16` rows and `3` columns.  \n",
    "> The entry in the `s`-th row and `a`-th column (`Q[s,a]`) contains the robot's *estimate* for the highest score it can possibly obtain in the game, if the game started in state `s`, and the robot selected action `a` for its first move.  \n",
    "\n",
    "For the first several games, the estimates in the array will be wildly inaccurate; but, the more experience the agent gets with gameplay, the more it is able to refine these estimates.\n",
    "\n",
    "It is relatively straightforward to determine what these estimates _should_ be, _if_ they are completely accurate.  For instance, consider `Q[3,1]`.  This value corresponds to state `3`, which corresponds to the case where the game starts with the lit LED at position `0`, and the robotic arm is at position `3` (*... please verify this in the section above!*).  If the robot selects action `1` as its first move in the game, then this corresponds to pushing in the current location, which results in a score of `-1`.  Then, at this point, if the agent plays optimally, it can obtain a final score of **at most** `-5`, obtained by selecting actions `0`, `0`, `0`, and `1` to end the game in four more moves.  The agent will try to estimate this value (`Q[3,1]=-5`), along with all other values in the array, from gameplay.  And - amazingly - it will refine these estimates while simultaneously trying to determine the optimal policy.  That is, it does not need to know the optimal policy *a priori* before forming these estimates!\n",
    "\n",
    "This array is deeply connected to the optimal policy.  What's particularly worth noting is that in the event that we have a perfect estimate of **all** of the values in `Q`, we can quickly use it to obtain the optimal policy.  To see this, suppose for now that the robot has a perfect estimate of all of the entries in the `Q` array.  Then, it can get the corresponding optimal action corresponding to any state `s` simply by looking at the `s`-th row in `Q`, or `Q[s,:]`.  As an example, if `s=3`, then `Q[3,:]=[-4, -5, -5]`.  \n",
    "> Then, to get the optimal action corresponding to state `s`, we need only select the action corresponding to the index that maximizes `Q[s,:]`.  \n",
    "\n",
    "In the case that `s=3`, we see that `-4` is the largest entry (which appears at index `0`), and so the optimal action is action `0` (which corresponds to moving left).  Take the time now to check that this lines up with both your intuition and the optimal policy above!  To understand more generally why this is the case, note that each entry in `Q[s,:]` records the final score if the robot follows the optimal policy, for all moves except (potentially) the first.  Then, one of the three available initial moves must be optimal, and it makes sense that the optimal move will correspond to the action (or index) that yields the largest maximum score in `Q[s,:]`.  (_For now, hopefully this is satisfying enough to press forward with implementation!  If you are interested in reading a proof of this fact, you're encouraged to check out the suggested resources at the end of this notebook._)\n",
    "\n",
    "\n",
    "\n",
    "### Implementation Details\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "\n",
    "def monte_carlo(env, num_episodes=70):\n",
    "    nLED = env.nLED                   # number of LEDs\n",
    "    nS = int(math.pow(nLED, 2))       # number of states\n",
    "    nA = env.nA                       # number of actions\n",
    "    \n",
    "    # initialize empty arrays\n",
    "    Q = np.zeros((nS, nA), dtype=float)\n",
    "    returns_sum = np.zeros((nS, nA), dtype=float)\n",
    "    returns_count = np.zeros((nS, nA), dtype=float)\n",
    "    \n",
    "    # loop over episodes\n",
    "    for i_episode in range(1, num_episodes+1):\n",
    "        \n",
    "        print(\"\\rEpisode {}/{}.\".format(i_episode, num_episodes), end=\"\")\n",
    "        sys.stdout.flush()\n",
    "        \n",
    "        # start the interaction\n",
    "        episode = []\n",
    "        led, arm = env.reset()\n",
    "        state = get_state(led, arm, nLED)        # initial state\n",
    "        \n",
    "        # select a random action\n",
    "        action = env.get_random_action()         # initial action \n",
    "        arm, reward, done = env.step(action)\n",
    "        episode.append((state, action, reward))  \n",
    "        \n",
    "        # finish the episode\n",
    "        for i in range(30):\n",
    "            p = math.pow(.985, i)\n",
    "            if not done:\n",
    "                # get state index\n",
    "                state = get_state(led, arm, nLED)\n",
    "                # select most profitable action\n",
    "                \n",
    "                if random.random() < p:\n",
    "                    Q_s = Q[state]\n",
    "                    best_actions = np.argwhere(Q_s == np.amax(Q_s)).flatten()\n",
    "                    action = random.choice(best_actions)\n",
    "                    #print(Q_s, action)\n",
    "                else:\n",
    "                    action = random.choice(np.arange(3))\n",
    "                    #print('random:', action)\n",
    "                \n",
    "                arm, reward, done = env.step(action)\n",
    "                episode.append((state, action, reward))\n",
    "            else:\n",
    "                break\n",
    "                \n",
    "        # use episode performance to update Q\n",
    "        sa_set = set([(x[0], x[1]) for x in episode])\n",
    "        for state, action in sa_set:\n",
    "            first_idx = min([i for i,x in enumerate(episode) if x[0] == state and x[1] == action])\n",
    "            returns_sum[state][action] += sum([x[2] for i,x in enumerate(episode[first_idx:])])\n",
    "            returns_count[state][action] += 1\n",
    "            Q[state][action] = returns_sum[state][action]/returns_count[state][action]\n",
    "            \n",
    "    return Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = monte_carlo(env, 7000)\n",
    "print('\\n')\n",
    "# print your agent's learned policy\n",
    "for state in range(Q.shape[0]):\n",
    "    led, arm = get_led_and_arm(state, env.nLED)\n",
    "    print('LED: %d | Arm: %d | State: %2d | Best Action: %s' % (led, arm, state, decipher_action(np.argmax(Q[state]))))\n",
    "\n",
    "# cleaner printing of learned policy\n",
    "print(np.argmax(Q, axis=1) == [1, 0, 0, 0, 2, 1, 0, 0, 2, 2, 1, 0, 2, 2, 2, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q[3,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Not Using / Learn More about RL\n",
    "\n",
    "but it will prove useful to define some additional terminology.\n",
    "\n",
    "In the field of RL, we refer to the equipment that controls the robot as the decision-making **agent**.  The agent must consider the **state** of the game when selecting **actions** that influence gameplay. \n",
    "\n",
    "We think of each game round as broken into discrete time steps.  At the start of a round, the agent observes the initial state of the game, which we denote by $s_0$.  Using the context provided by the state, the agent has to choose an appropriate action $a_0$ in response.  This action likely changes the state of the game, and we denote the following game state as $s_1$.\n",
    "\n",
    "After the agent chooses an action, the score of the game is deducted by one point.  This negative reinforcement is used to signal to the agent that it should try to end the game as quickly as possible.  \n",
    "\n",
    "We can translate this change in game score to the traditional RL framework by saying that the agent received a **reward** of -1.  In particular, we can say the agent received a reward $r_1=-1$ as a result of selecting action $a_0$ after observing state $s_0$.\n",
    "\n",
    "We refer to a complete game round, from start to finish, as an **episode**.  Each episode evolves as a sequence of states, actions, and rewards \n",
    "\n",
    "$$\n",
    "(s_0, a_0, r_1, \\ldots, s_t, a_t, r_{t+1}, \\ldots, s_{T-1}, a_{T-1}, r_T, s_T),\n",
    "$$ \n",
    "\n",
    "where $T$ is the final time step, and $s_T$ is the final game state (where the LED has been turned off by the robotic arm).  The final score of the agent can be identified as the sum \n",
    "$$\n",
    "\\sum_{t=1}^Tr_t = r_1 + \\ldots + r_T,\n",
    "$$\n",
    "which we also refer to as the **cumulative reward** collected by the agent over the course of the episode.  Note that this cumulative reward is equivalent to the robot's final score at the end of the game.\n",
    "\n",
    "Our goal is to teach the agent to develop a game-playing strategy to maximize cumulative reward, and you will learn how to design an algorithm to accomplish this soon.  "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
