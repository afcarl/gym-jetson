{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Reinforcement Learning with NVIDIA Jetson TX2\n",
    "\n",
    "In this session, you will use a branch of machine learning called **reinforcement learning** (RL) to teach a robot to play a game."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Game\n",
    "\n",
    "The setup has four LEDs.  We enumerate the LEDs starting with zero, so that the yellow LED is at position `0`, the red LED is at position `1`, and so on.  Each LED is connected to a button that is used to turn it off.  \n",
    "\n",
    "![LEDs](images/LEDs.png)\n",
    "\n",
    "The game round begins when **one** of the four LEDs is turned on, and the robotic arm starts at a position hovering over one of the four buttons.  The score always starts at zero.\n",
    "\n",
    "At each point in the game, the robot has three potential movements (or **actions**) at its disposal.  It can:\n",
    "- `0` - move **L**eft one position,\n",
    "- `1` - stay at the current position and **P**ush, or\n",
    "- `2` - move **R**ight one position.\n",
    "\n",
    "After the robot chooses an action, its score is deducted one point.  While most actions have intuitive effects, there are a few special cases that are worth mentioning:\n",
    "- The lit LED is turned off when the robot pushes its corresponding button, but pushing buttons connected to unlit LEDs has no effect (i.e., they are never turned on by the robot).  \n",
    "- If the robotic arm is hovering over the leftmost button at location `0` and decides to move left, we imagine that the arm hits an imaginary wall, and the arm stays where it is.  Likewise, if the arm is hovering over the button at position `3` and decides to move right, then at the next point in the game, the arm will have maintained its position at location `3` (the rightmost location in the line).\n",
    "\n",
    "The round ends when the robot pushes the correct button to turn off the lit LED.  Your goal in this notebook is to implement an algorithm that can learn from gameplay the optimal strategy to attain the highest (or least negative) score at each round.  To accomplish this, your robot will need to learn how to turn off the lit LED in as few movements as possible.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Motivating Reinforcement Learning\n",
    "\n",
    "You may have noticed that the winning game strategy is very straightforward to hard code.  With this in mind, if you are new to RL, you might wonder why an RL technique would not be overkill.\n",
    "\n",
    "And the fact is that in the field of RL, it is very common to use simple games with well-defined rules **to build intuition** for how to design algorithms to best accomplish more complex tasks.  \n",
    "\n",
    "In the RL setting, we assume that the robot does not have the domain knowledge to know what \"left\", \"push\", or \"right\" means.  The robot only knows that it has three possible actions, and it needs to figure out how to select from these actions to consistently attain the highest possible score.  \n",
    "\n",
    "You can think of the robot as a computer player with full access to a keyboard that only contains three keys, where you further assume the player does not know any of the game controls.  Despite this missing information, the player must nonetheless learn how to beat the game.  \n",
    "\n",
    "And, similar to how you can imagine the human player would learn, the robot will closely watch its score in the game to gauge how well it is doing.  If it is not performing well, it will amend its strategy to do better.  \n",
    "\n",
    "The algorithm that you will use to play the game is remarkably intuitive.  The robot will learn primarily through trial-and-error, while using its score as a feedback mechanism to hone its strategy.  Its initial behavior will be incredibly random, as it tries out many different moves (or actions) in the game, to collect useful results.  You will use a reinforcement learning algorithm to train your robot to leverage its experience to settle on a well-informed strategy that attains a relatively high score.   \n",
    "\n",
    "For now, in the next section, we will examine how the robot performs, if it chooses random actions at each time step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Investigate Random Behavior\n",
    "\n",
    "We have written a simple simulator that you can use to see how the robot should perform, if it selects random actions for the entirety of the game.  Of course, your robot will learn to perform much better!\n",
    "\n",
    "Run the code cell below to have the robot play the game for 2 separate rounds.  When parsing the output, remember that the starting conditions of the game are random!\n",
    "\n",
    "Each round has corresponding output that looks somewhat like the snippet below:\n",
    "```\n",
    "Starting Round 1 ...\n",
    " LED: 0    | Arm: 1 | Score:    0 \n",
    " Action: P | Arm: 1 | Score:   -1 \n",
    " Action: L | Arm: 0 | Score:   -2 \n",
    " Action: P | Arm: 0 | Score:   -3 \n",
    "FINAL SCORE:  -3\n",
    "-----------------------------------\n",
    "```\n",
    "\n",
    "In the sample snippet above:\n",
    "- When the game round was initiated, the lit LED was at position `0`, and the arm was at position `1`.  The game score at the beginning of a round is always zero. \n",
    "- The robot's first choice of action was to **P**ush in the current location; as a result, its score was deducted one point, and the LEDs were unaffected (i.e., the LED at position `0` remains lit, while all of the other LEDs are unlit).  \n",
    "- The robot's next choice was to move **L**eft, so it moved to position `0` and the score was deducted another point.\n",
    "- Then, the robot decided to **P**ush in location `0`, and the game score decreased to `-3`.  At this point, the game ended, because the final action turned off the lit LED. \n",
    "- In this case, the final score received at the end of the game was `-1` + `-1` + `-1` = `-3`.\n",
    "\n",
    "Take the time now to understand the `JetsonEnv` class in **jetson_env.py**.  Note that the simulation encodes each of the possible actions as an integer (one of `0`, `1`, or `2`), and to get the corresponding more interpretable action label (`L`, `P`, or `R`), we use the `decipher_action` function below. \n",
    "\n",
    "Later in this notebook, you will use the `JetsonEnv` class to simulate games to teach your robot!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Round 1 ...\n",
      " LED: 2    | Arm: 0 | Score:    0 \n",
      " Action: L | Arm: 0 | Score:   -1 \n",
      " Action: P | Arm: 0 | Score:   -2 \n",
      " Action: L | Arm: 0 | Score:   -3 \n",
      " Action: R | Arm: 1 | Score:   -4 \n",
      " Action: R | Arm: 2 | Score:   -5 \n",
      " Action: L | Arm: 1 | Score:   -6 \n",
      " Action: R | Arm: 2 | Score:   -7 \n",
      " Action: L | Arm: 1 | Score:   -8 \n",
      " Action: R | Arm: 2 | Score:   -9 \n",
      " Action: R | Arm: 3 | Score:  -10 \n",
      " Action: R | Arm: 3 | Score:  -11 \n",
      " Action: P | Arm: 3 | Score:  -12 \n",
      " Action: L | Arm: 2 | Score:  -13 \n",
      " Action: R | Arm: 3 | Score:  -14 \n",
      " Action: R | Arm: 3 | Score:  -15 \n",
      " Action: L | Arm: 2 | Score:  -16 \n",
      " Action: R | Arm: 3 | Score:  -17 \n",
      " Action: L | Arm: 2 | Score:  -18 \n",
      " Action: P | Arm: 2 | Score:  -19 \n",
      "FINAL SCORE:  -19\n",
      "-----------------------------------\n",
      "Starting Round 2 ...\n",
      " LED: 3    | Arm: 0 | Score:    0 \n",
      " Action: P | Arm: 0 | Score:   -1 \n",
      " Action: P | Arm: 0 | Score:   -2 \n",
      " Action: P | Arm: 0 | Score:   -3 \n",
      " Action: L | Arm: 0 | Score:   -4 \n",
      " Action: L | Arm: 0 | Score:   -5 \n",
      " Action: P | Arm: 0 | Score:   -6 \n",
      " Action: L | Arm: 0 | Score:   -7 \n",
      " Action: R | Arm: 1 | Score:   -8 \n",
      " Action: L | Arm: 0 | Score:   -9 \n",
      " Action: P | Arm: 0 | Score:  -10 \n",
      " Action: R | Arm: 1 | Score:  -11 \n",
      " Action: P | Arm: 1 | Score:  -12 \n",
      " Action: P | Arm: 1 | Score:  -13 \n",
      " Action: P | Arm: 1 | Score:  -14 \n",
      " Action: L | Arm: 0 | Score:  -15 \n",
      " Action: R | Arm: 1 | Score:  -16 \n",
      " Action: R | Arm: 2 | Score:  -17 \n",
      " Action: R | Arm: 3 | Score:  -18 \n",
      " Action: R | Arm: 3 | Score:  -19 \n",
      " Action: R | Arm: 3 | Score:  -20 \n",
      " Action: L | Arm: 2 | Score:  -21 \n",
      " Action: R | Arm: 3 | Score:  -22 \n",
      " Action: R | Arm: 3 | Score:  -23 \n",
      " Action: L | Arm: 2 | Score:  -24 \n",
      " Action: L | Arm: 1 | Score:  -25 \n",
      " Action: L | Arm: 0 | Score:  -26 \n",
      " Action: R | Arm: 1 | Score:  -27 \n",
      " Action: P | Arm: 1 | Score:  -28 \n",
      " Action: L | Arm: 0 | Score:  -29 \n",
      " Action: L | Arm: 0 | Score:  -30 \n",
      " Action: P | Arm: 0 | Score:  -31 \n",
      " Action: R | Arm: 1 | Score:  -32 \n",
      " Action: P | Arm: 1 | Score:  -33 \n",
      " Action: R | Arm: 2 | Score:  -34 \n",
      " Action: L | Arm: 1 | Score:  -35 \n",
      " Action: L | Arm: 0 | Score:  -36 \n",
      " Action: R | Arm: 1 | Score:  -37 \n",
      " Action: L | Arm: 0 | Score:  -38 \n",
      " Action: L | Arm: 0 | Score:  -39 \n",
      " Action: R | Arm: 1 | Score:  -40 \n",
      " Action: R | Arm: 2 | Score:  -41 \n",
      " Action: P | Arm: 2 | Score:  -42 \n",
      " Action: R | Arm: 3 | Score:  -43 \n",
      " Action: R | Arm: 3 | Score:  -44 \n",
      " Action: P | Arm: 3 | Score:  -45 \n",
      "FINAL SCORE:  -45\n",
      "-----------------------------------\n"
     ]
    }
   ],
   "source": [
    "from jetson_env import JetsonEnv\n",
    "\n",
    "# use a Python dictionary to decode the actions\n",
    "action_dict = {0: 'L', 1: 'P', 2: 'R'}\n",
    "def decipher_action(a):\n",
    "    return action_dict[a]\n",
    "\n",
    "# create a new environment\n",
    "env = JetsonEnv()\n",
    "\n",
    "# interact with the environment\n",
    "for i_episode in range(1, 3):\n",
    "    print('Starting Round %d ...' % i_episode)\n",
    "    # reset the lit LED, arm position, and score\n",
    "    led, arm = env.reset()\n",
    "    score = 0\n",
    "    print(' LED: %d    | Arm: %d | Score: %4d ' % (led, arm, score))\n",
    "    while True:\n",
    "        action = env.get_random_action() # select a random action\n",
    "        arm, reward, done = env.step(action)\n",
    "        score += reward\n",
    "        print(' Action: %s | Arm: %d | Score: %4d ' % (decipher_action(action), arm, score))\n",
    "        if done:\n",
    "            print('FINAL SCORE: ', score)\n",
    "            print('-'*35)\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## States, Actions, and Optimal Policies\n",
    "\n",
    "As discovered above, there are **three possible actions**, corresponding to:\n",
    "- `0` - moving **L**eft, \n",
    "- `1` - staying and **P**ushing in the current position, and\n",
    "- `2` - moving **R**ight.\n",
    "\n",
    "The **total number of possible game states is $4^2 = 16$**, where there is a state for each possible combination of arm position and lit LED position. \n",
    "\n",
    "To avoid having to deal with two different numbers when referencing the state, we define the `get_state` function below that maps each possible combination of arm position (`arm`) and lit LED position (`led`) to an integer from `0` to `15`, which we refer to as the corresponding state (`state`).  \n",
    "\n",
    "In your upcoming implementation, the state should always be encoded as a number from `0` to `15`, but you can get the corresponding arm position (`arm`) and lit LED position (`led`) by passing the state (`state`) into the `get_led_and_arm` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LED: 0 | Arm: 0 | State:  0 \n",
      "LED: 0 | Arm: 1 | State:  1 \n",
      "LED: 0 | Arm: 2 | State:  2 \n",
      "LED: 0 | Arm: 3 | State:  3 \n",
      "LED: 1 | Arm: 0 | State:  4 \n",
      "LED: 1 | Arm: 1 | State:  5 \n",
      "LED: 1 | Arm: 2 | State:  6 \n",
      "LED: 1 | Arm: 3 | State:  7 \n",
      "LED: 2 | Arm: 0 | State:  8 \n",
      "LED: 2 | Arm: 1 | State:  9 \n",
      "LED: 2 | Arm: 2 | State: 10 \n",
      "LED: 2 | Arm: 3 | State: 11 \n",
      "LED: 3 | Arm: 0 | State: 12 \n",
      "LED: 3 | Arm: 1 | State: 13 \n",
      "LED: 3 | Arm: 2 | State: 14 \n",
      "LED: 3 | Arm: 3 | State: 15 \n"
     ]
    }
   ],
   "source": [
    "def get_led_and_arm(state, nLED=4):\n",
    "    arm = state % nLED\n",
    "    led = int(((state-arm))/nLED)\n",
    "    return led, arm\n",
    "\n",
    "def get_state(led, arm, nLED=4):\n",
    "    state = led*nLED + arm\n",
    "    return state\n",
    "\n",
    "for state in range(16):\n",
    "    led, arm = get_led_and_arm(state)\n",
    "    print('LED: %d | Arm: %d | State: %2d ' % (led, arm, state))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of your robot is to find - for each possible game state - the best action that the robot should take from that state, towards its goal of maximizing the game score.  We will think of this as a lookup table that the robot can consult when selecting actions, and we refer to it as an **optimal policy**.\n",
    "\n",
    "For instance, consider the case that the game starts in state `0`.  This state corresponds to arm position `0` and lit LED position `0`.  In this case, the robot should select action **P**ush, to end the game with a final score of `-1` immediately after.\n",
    "\n",
    "Likewise, state `1` corresponds to arm position `1` and lit LED position `0`.  In this case, the robot should decide to move **L**eft as the best initial move.  (This way, the robot can select to **P**ush at the next step and end the game with a best final score of `-2`.)\n",
    "\n",
    "Take the time now to look at the printed optimal policy below.  Check to make sure that you can see why these actions are optimal, in the context of their corresponding game states!\n",
    "\n",
    "```\n",
    "LED: 0 | Arm: 0 | State:  0 | Best Action: P\n",
    "LED: 0 | Arm: 1 | State:  1 | Best Action: L\n",
    "LED: 0 | Arm: 2 | State:  2 | Best Action: L\n",
    "LED: 0 | Arm: 3 | State:  3 | Best Action: L\n",
    "LED: 1 | Arm: 0 | State:  4 | Best Action: R\n",
    "LED: 1 | Arm: 1 | State:  5 | Best Action: P\n",
    "LED: 1 | Arm: 2 | State:  6 | Best Action: L\n",
    "LED: 1 | Arm: 3 | State:  7 | Best Action: L\n",
    "LED: 2 | Arm: 0 | State:  8 | Best Action: R\n",
    "LED: 2 | Arm: 1 | State:  9 | Best Action: R\n",
    "LED: 2 | Arm: 2 | State: 10 | Best Action: P\n",
    "LED: 2 | Arm: 3 | State: 11 | Best Action: L\n",
    "LED: 3 | Arm: 0 | State: 12 | Best Action: R\n",
    "LED: 3 | Arm: 1 | State: 13 | Best Action: R\n",
    "LED: 3 | Arm: 2 | State: 14 | Best Action: R\n",
    "LED: 3 | Arm: 3 | State: 15 | Best Action: P\n",
    "```\n",
    "\n",
    "Once the robot has determined the optimal policy, it can reference it when playing the game, in order to consistently attain the highest possible score.  For instance, if the robot is presented with a situation in the game where the lit LED is at position 2, and its arm is at position 1, it need only find the line corresponding to state 9 in the table, where it will see that the best action is to go left.\n",
    "\n",
    "Next, you will implement a method known as **Monte Carlo with Exploring Starts** to guide your robot to obtain this optimal policy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monte Carlo with Exploring Starts\n",
    "\n",
    "\n",
    "agent doesn't know optimal policy ... and won't try to estimate directly ... instead will estimate the value of that optimal policy ... as agent plays the game, will keep track of this estimate for each possible game state of how advantageous it is to pick any action ... and then henceforth select optimal actions ...\n",
    "\n",
    "As part of this algorithm, the agent maintains a numpy array `Q` with 16 rows and 3 columns.  The entry in the `s`-th row and `a`-th column (`Q[s,a]`) keeps contains the agent's expected final game score, if the game starts in state `s`, the agent selects action `a`, and then the agent refers to the optimal policy to select actions until the game ends.\n",
    "\n",
    "`Q[s,a]` contains ...\n",
    "\n",
    "#### STILL NEED TO WRITE ... the agent's estimate for its final score, if the game starts in state `s`, the agent selects action `a`, and then all future actions are selected using the optimal policy.\n",
    "\n",
    "*This description of Monte Carlo Exploring Starts still needs to be fleshed out.  The plan is that the attendee will code everything from scratch.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "\n",
    "def monte_carlo(env, num_episodes=70):\n",
    "    nLED = env.nLED                   # number of LEDs\n",
    "    nS = int(math.pow(nLED, 2))       # number of states\n",
    "    nA = env.nA                       # number of actions\n",
    "    \n",
    "    # initialize empty arrays\n",
    "    Q = np.zeros((nS, nA), dtype=float)\n",
    "    returns_sum = np.zeros((nS, nA), dtype=float)\n",
    "    returns_count = np.zeros((nS, nA), dtype=float)\n",
    "    \n",
    "    # loop over episodes\n",
    "    for i_episode in range(1, num_episodes+1):\n",
    "        \n",
    "        print(\"\\rEpisode {}/{}.\".format(i_episode, num_episodes), end=\"\")\n",
    "        sys.stdout.flush()\n",
    "        \n",
    "        # start the interaction\n",
    "        episode = []\n",
    "        led, arm = env.reset()\n",
    "        state = get_state(led, arm, nLED)        # initial state\n",
    "        \n",
    "        # select a random action\n",
    "        action = env.get_random_action()         # initial action \n",
    "        arm, reward, done = env.step(action)\n",
    "        episode.append((state, action, reward))  \n",
    "        \n",
    "        # finish the episode\n",
    "        for i in range(30):\n",
    "            p = math.pow(.985, i)\n",
    "            if not done:\n",
    "                # get state index\n",
    "                state = get_state(led, arm, nLED)\n",
    "                # select most profitable action\n",
    "                \n",
    "                if random.random() < p:\n",
    "                    Q_s = Q[state]\n",
    "                    best_actions = np.argwhere(Q_s == np.amax(Q_s)).flatten()\n",
    "                    action = random.choice(best_actions)\n",
    "                    #print(Q_s, action)\n",
    "                else:\n",
    "                    action = random.choice(np.arange(3))\n",
    "                    #print('random:', action)\n",
    "                \n",
    "                arm, reward, done = env.step(action)\n",
    "                episode.append((state, action, reward))\n",
    "            else:\n",
    "                break\n",
    "                \n",
    "        # use episode performance to update Q\n",
    "        sa_set = set([(x[0], x[1]) for x in episode])\n",
    "        for state, action in sa_set:\n",
    "            first_idx = min([i for i,x in enumerate(episode) if x[0] == state and x[1] == action])\n",
    "            returns_sum[state][action] += sum([x[2] for i,x in enumerate(episode[first_idx:])])\n",
    "            returns_count[state][action] += 1\n",
    "            Q[state][action] = returns_sum[state][action]/returns_count[state][action]\n",
    "            \n",
    "    return Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 70/70.\n",
      "\n",
      "LED: 0 | Arm: 0 | State:  0 | Best Action: P\n",
      "LED: 0 | Arm: 1 | State:  1 | Best Action: L\n",
      "LED: 0 | Arm: 2 | State:  2 | Best Action: L\n",
      "LED: 0 | Arm: 3 | State:  3 | Best Action: L\n",
      "LED: 1 | Arm: 0 | State:  4 | Best Action: R\n",
      "LED: 1 | Arm: 1 | State:  5 | Best Action: P\n",
      "LED: 1 | Arm: 2 | State:  6 | Best Action: L\n",
      "LED: 1 | Arm: 3 | State:  7 | Best Action: L\n",
      "LED: 2 | Arm: 0 | State:  8 | Best Action: R\n",
      "LED: 2 | Arm: 1 | State:  9 | Best Action: R\n",
      "LED: 2 | Arm: 2 | State: 10 | Best Action: P\n",
      "LED: 2 | Arm: 3 | State: 11 | Best Action: L\n",
      "LED: 3 | Arm: 0 | State: 12 | Best Action: R\n",
      "LED: 3 | Arm: 1 | State: 13 | Best Action: R\n",
      "LED: 3 | Arm: 2 | State: 14 | Best Action: R\n",
      "LED: 3 | Arm: 3 | State: 15 | Best Action: P\n",
      "[ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True]\n"
     ]
    }
   ],
   "source": [
    "Q = monte_carlo(env, 70)\n",
    "print('\\n')\n",
    "# print your agent's learned policy\n",
    "for state in range(Q.shape[0]):\n",
    "    led, arm = get_led_and_arm(state, env.nLED)\n",
    "    print('LED: %d | Arm: %d | State: %2d | Best Action: %s' % (led, arm, state, decipher_action(np.argmax(Q[state]))))\n",
    "\n",
    "# cleaner printing of learned policy\n",
    "print(np.argmax(Q, axis=1) == [1, 0, 0, 0, 2, 1, 0, 0, 2, 2, 1, 0, 2, 2, 2, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ -2.2       ,  -2.375     ,  -4.        ,  -5.85714286,\n",
       "        -6.66666667,  -4.33333333,  -2.        ,  -3.        ,\n",
       "       -11.        ,  -7.        ,  -8.5       ,  -3.18181818,\n",
       "       -13.33333333, -12.33333333,  -5.        ,  -3.        ])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q[:,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Not Using / Learn More about RL\n",
    "\n",
    "but it will prove useful to define some additional terminology.\n",
    "\n",
    "In the field of RL, we refer to the equipment that controls the robot as the decision-making **agent**.  The agent must consider the **state** of the game when selecting **actions** that influence gameplay. \n",
    "\n",
    "We think of each game round as broken into discrete time steps.  At the start of a round, the agent observes the initial state of the game, which we denote by $s_0$.  Using the context provided by the state, the agent has to choose an appropriate action $a_0$ in response.  This action likely changes the state of the game, and we denote the following game state as $s_1$.\n",
    "\n",
    "After the agent chooses an action, the score of the game is deducted by one point.  This negative reinforcement is used to signal to the agent that it should try to end the game as quickly as possible.  \n",
    "\n",
    "We can translate this change in game score to the traditional RL framework by saying that the agent received a **reward** of -1.  In particular, we can say the agent received a reward $r_1=-1$ as a result of selecting action $a_0$ after observing state $s_0$.\n",
    "\n",
    "We refer to a complete game round, from start to finish, as an **episode**.  Each episode evolves as a sequence of states, actions, and rewards \n",
    "\n",
    "$$\n",
    "(s_0, a_0, r_1, \\ldots, s_t, a_t, r_{t+1}, \\ldots, s_{T-1}, a_{T-1}, r_T, s_T),\n",
    "$$ \n",
    "\n",
    "where $T$ is the final time step, and $s_T$ is the final game state (where the LED has been turned off by the robotic arm).  The final score of the agent can be identified as the sum \n",
    "$$\n",
    "\\sum_{t=1}^Tr_t = r_1 + \\ldots + r_T,\n",
    "$$\n",
    "which we also refer to as the **cumulative reward** collected by the agent over the course of the episode.  Note that this cumulative reward is equivalent to the robot's final score at the end of the game.\n",
    "\n",
    "Our goal is to teach the agent to develop a game-playing strategy to maximize cumulative reward, and you will learn how to design an algorithm to accomplish this soon.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
